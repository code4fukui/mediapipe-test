<!DOCTYPE html><html lang="ja"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width"><link rel="icon" href="data:">

<!-- Mediapipe Pose -->
<!--
<script src="https://cdn.jsdelivr.net/npm/@mediapipe/pose"></script>
-->
<script src="https://cdn.jsdelivr.net/npm/@mediapipe/pose/pose.js" crossorigin="anonymous"></script>

<!-- TensorFlow.js -->
<script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs"></script>

<!-- TensorFlow.js Models (YOLO model in this case) -->
<script src="https://cdn.jsdelivr.net/npm/@tensorflow-models/coco-ssd"></script>
<link rel="stylesheet" href="style.css">

</head><body>

<body>
<h1>MediaPipe Pose + Coco-SSD test</h1>
<div class="container">
  <video id="videoElement" playsinline></video>
  <canvas id="canvasElement"></canvas>
  <div class="landmark-grid-container"></div>
</div>
<label><input type="checkbox" id="showimg">show original image</label>
<!--
<label><input type="checkbox" id="segimg">overwrite by segmentation</label>
-->
<label><input type="checkbox" id="mirrormode">mirror mode</label>
<label><input type="checkbox" id="backcameramode">backcamera mode</label>
<hr>
<footer>
<a href=./>demo index</a><br>
lib: <a href="https://chuoling.github.io/mediapipe/solutions/pose.html">Pose - mediapipe</a><br>
LIB: <a href="https://github.com/tensorflow/tfjs-models/tree/master/coco-ssd">tfjs-models/coco-ssd at master Â· tensorflow/tfjs-models</a><br>
src: <a href="https://github.com/code4fukui/mediapipe-test/">mediapipe-test</a><br>
</footer>


<script type="module">
import { Camera } from "https://code4fukui.github.io/Camera/Camera.js";

const g = canvasElement.getContext("2d");

const pose = new Pose({ locateFile: (file) => `https://cdn.jsdelivr.net/npm/@mediapipe/pose/${file}` });

pose.setOptions({
  staticImageMode: false,
  modelComplexity: 1,
  smoothLandmarks: true,
  enableSegmentation: false,
  smoothSegmentation: false,
  minDetectionConfidence: 0.5,
  minTrackingConfidence: 0.5
});

// Load the COCO-SSD model
let model;
cocoSsd.load().then((loadedModel) => {
  model = loadedModel;
});

// Function to perform object detection and pose estimation
async function detectFrame() {
  const canvas = canvasElement;
  const video = videoElement;

  g.save();
  if (mirrormode.checked) {
    g.scale(-1, 1);
    g.translate(-canvas.width, 0);
  }

  // Clear canvas
  g.clearRect(0, 0, canvas.width, canvas.height);
  if (showimg.checked) {
    g.drawImage(video, 0, 0, canvas.width, canvas.height);
  }
  
  if (!model) return;
  const predictions = await model.detect(video);

  // Filter detections to only include 'person' class
  const people = predictions.filter((pred) => pred.class === 'person');

  const waitPose = async (image) => {
    return new Promise(async (resolve) => {
      pose.onResults(resolve);
      pose.send({ image });
    });
  };
  // Function to draw pose landmarks
  function drawLandmarks(landmarks, offsetX, offsetY, width, height) {
    g.fillStyle = 'red';
    //g.lineWidth = 2;

    for (const landmark of landmarks) {
      const { x, y } = landmark;
      g.beginPath();
      g.arc(x * width + offsetX, y * height + offsetY, 5, 0, 2 * Math.PI);
      g.fill();
    }
  }

  // Loop through detected persons
  for (const person of people) {
    const [x, y, width, height] = person.bbox;

    // Draw bounding box
    g.strokeStyle = 'green';
    g.lineWidth = 2;
    g.strokeRect(x, y, width, height);
    g.fillStyle = 'green';
    g.fillText(`Person`, x, y - 5);

    //if (true) continue;
    // Crop the video frame for each detected person
    const personImage = document.createElement('canvas');
    personImage.width = width;
    personImage.height = height;
    personImage.getContext('2d').drawImage(video, x, y, width, height, 0, 0, width, height);

    // Convert the cropped image to ImageData
    const imageData = personImage.getContext('2d').getImageData(0, 0, width, height);
    
    const results = await waitPose(imageData);
    if (results.poseLandmarks) {
      drawLandmarks(results.poseLandmarks, x, y, width, height);
    }
  }
  g.restore();
};


const camera = new Camera(videoElement, {
  onFrame: async () => {
    //const dpi = devicePixelRatio;
    const dpi = 1;
    canvasElement.width = videoElement.videoWidth * dpi;
    canvasElement.height = videoElement.videoHeight * dpi;
    //await pose.send({ image: videoElement });
    await detectFrame();
  },
  width: 1280,
  height: 720,
  backcamera: backcameramode.checked,
});
camera.start();
backcameramode.onchange = () => camera.flip();
</script>

</body></html>
